{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgV9mtK0BvDhalQcsswHaO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahsan3274/Fellowship.ai/blob/main/ai_fellowship.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyBqxzxkfcgy",
        "outputId": "d41623f5-556a-4f16-ede3-2fdbdc508eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: nasquamesse\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to ./imdb-dataset-of-50k-movie-reviews\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25.7M/25.7M [00:00<00:00, 115MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas\n",
        "import opendatasets as od\n",
        "od.download(\n",
        "    \"https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df['sentiment'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0I6iOO2NSdI",
        "outputId": "e8bea762-986b-439b-d1e9-33ca0e37c340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n",
            "None\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Remove stop words and lemmatize the text\n",
        "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the reviews\n",
        "df['review'] = df['review'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nfSxTe2OdJ2",
        "outputId": "f8e0ec27-5c6d-4ab8-a0bc-42c488479f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the reviews\n",
        "X = tfidf.fit_transform(df['review']).toarray()\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n"
      ],
      "metadata": {
        "id": "emphNw3aOtN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "YpEUJcY1O23u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Display classification report\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Display confusion matrix\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIJs19W1O_1i",
        "outputId": "9e35f424-5c98-4663-cb90-90084ffec842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8866\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.88      4961\n",
            "           1       0.88      0.90      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4327  634]\n",
            " [ 500 4539]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM networks are specifically designed to handle sequential data and maintain long-term dependencies. In the context of sentiment analysis, understanding the sequence of words and their context within a review is crucial for accurately determining sentiment. Unlike traditional neural networks, LSTMs can capture the temporal dependencies between words in a sentence. Standard Recurrent Neural Networks (RNNs) struggle with the vanishing gradient problem, making it difficult to learn long-term dependencies. LSTMs overcome this issue through their unique architecture, which includes gates (input, forget, and output gates) that regulate the flow of information. This allows LSTMs to remember relevant information over long sequences and forget irrelevant details, leading to better performance on tasks requiring context retention. Sentiment analysis relies heavily on context to interpret the sentiment of words correctly. For example, the meaning of \"not bad\" is different from \"bad.\" LSTM networks excel at capturing such contextual nuances by maintaining an internal state that evolves as they process each word in the sequence. This ability to understand the context makes LSTMs particularly well-suited for sentiment analysis."
      ],
      "metadata": {
        "id": "LKwWfKUT2O9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['review'])\n",
        "X = tokenizer.texts_to_sequences(df['review'])\n",
        "X = pad_sequences(X, maxlen=500)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model with Dropout layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=500))\n",
        "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Use early stopping to avoid overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "\n",
        "# Train the model with regularization and early stopping\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2htQrHTPFWK",
        "outputId": "3639c6a4-ce45-4382-a45b-08d9afab824f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 1126s 2s/step - loss: 0.3596 - accuracy: 0.8408 - val_loss: 0.2764 - val_accuracy: 0.8875\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 1096s 2s/step - loss: 0.2511 - accuracy: 0.9006 - val_loss: 0.3074 - val_accuracy: 0.8654\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 1070s 2s/step - loss: 0.2109 - accuracy: 0.9184 - val_loss: 0.2991 - val_accuracy: 0.8865\n",
            "313/313 [==============================] - 54s 172ms/step - loss: 0.2764 - accuracy: 0.8875\n",
            "Accuracy: 0.8875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results show that the model performs well on the training data, with continuous improvements in training accuracy and decreases in training loss. The early stopping mechanism worked as intended, stopping training when the validation loss did not improve for two consecutive epochs, thereby preventing further overfitting. The validation accuracy peaked early and remained high, indicating that the model maintained its generalization ability throughout the training. The slight fluctuations in validation performance suggest that while the model is robust, there might be room for fine-tuning or additional regularization techniques to improve stability and prevent any overfitting. Overall, achieving an accuracy of 88.75% on the test set is a strong indicator that the LSTM model is effective for sentiment analysis on the IMDB dataset."
      ],
      "metadata": {
        "id": "CPemzegj3THZ"
      }
    }
  ]
}